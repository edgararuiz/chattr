<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Interact with local models • chattr</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Interact with local models">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">chattr</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/backend-databricks.html">Interact with Databricks foundation models</a></li>
    <li><a class="dropdown-item" href="../articles/backend-llamagpt.html">Interact with local models</a></li>
    <li><a class="dropdown-item" href="../articles/copilot-chat.html">Interact with GitHub Copilot Chat</a></li>
    <li><a class="dropdown-item" href="../articles/openai-gpt.html">Interact with OpenAI GPT models</a></li>
    <li><a class="dropdown-item" href="../articles/other-interfaces.html">Other interfaces</a></li>
    <li><a class="dropdown-item" href="../articles/prompt_defaults.html">Modify prompt enhancements</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/mlverse/chattr/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Interact with local models</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/mlverse/chattr/blob/main/vignettes/backend-llamagpt.Rmd" class="external-link"><code>vignettes/backend-llamagpt.Rmd</code></a></small>
      <div class="d-none name"><code>backend-llamagpt.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="intro">Intro<a class="anchor" aria-label="anchor" href="#intro"></a>
</h2>
<p><a href="https://github.com/kuvaus/LlamaGPTJ-chat" class="external-link">LlamaGPT-Chat</a>
is a command line chat application. It integrates with the LLM via C++.
This means that it will only work with LLM’s that have been in the same
language. There are few available today that are free to download and
use. Because they are in C++, the models are able to run locally on your
computer, even if there is no GPU available. Most of the models are
quite fast in their responses. By integrating with LlamaGPT-Chat, able
to access multiple types of LLM’s with only one additional back-end
within <code>chattr</code>.</p>
</div>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>To get LlamaGPT-Chat working you machine you will need two
things:</p>
<ol style="list-style-type: decimal">
<li><p>A version of LlamaGPT-Chat that works in your computer</p></li>
<li><p>An LLM file that works with with the chat program</p></li>
</ol>
<div class="section level3">
<h3 id="install-llamagpt-chat">Install LLamaGPT-Chat<a class="anchor" aria-label="anchor" href="#install-llamagpt-chat"></a>
</h3>
<p>LlamaGPT-Chat will need a “compiled binary” that is specific to your
Operating System. For example, for Windows, a compiled binary should be
an <em>.exe</em> file.</p>
<p>The GitHub repository offers pre-compiled binaries that you can
download and use: <a href="https://github.com/kuvaus/LlamaGPTJ-chat/releases" class="external-link">Releases</a>.
Depending on the system’s security, the pre-compiled program may blocked
from running. If that is the case, you will need to “build” the program
in your computer. The instructions to do this are <a href="https://github.com/kuvaus/LlamaGPTJ-chat#download" class="external-link">here</a>.</p>
</div>
<div class="section level3">
<h3 id="llm-file">LLM file<a class="anchor" aria-label="anchor" href="#llm-file"></a>
</h3>
<p>The GitHub repository contains a <a href="https://github.com/kuvaus/LlamaGPTJ-chat#gpt-j-llama-and-mpt-models" class="external-link">list
of compatible models</a>. Download at least one of the models, and make
note of where it was saved to in your computer.</p>
</div>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<p>To start, instruct <code>chattr</code> to switch the LLamaGPT
back-end using <code><a href="../reference/chattr_use.html">chattr_use()</a></code></p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/mlverse/chattr" class="external-link">chattr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/chattr_use.html">chattr_use</a></span><span class="op">(</span><span class="st">"llamagpt"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── chattr</span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Provider:</span> <span style="color: #0000BB;">LlamaGPT</span></span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Path/URL:</span> <span style="color: #0000BB;">~/LlamaGPTJ-chat/build/bin/chat</span></span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Model:</span> <span style="color: #0000BB;">~/ggml-gpt4all-j-v1.3-groovy.bin</span></span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Label:</span> <span style="color: #0000BB;">GPT4ALL 1.3 (LlamaGPT)</span></span></span></code></pre></div>
<p>If this is the first time you use this interface, confirm that the
path of to the compiled program, and the model matches to what you have
in your machine. To check that use <code><a href="../reference/chattr_defaults.html">chattr_defaults()</a></code></p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/chattr_defaults.html">chattr_defaults</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; <span style="color: #00BBBB;">──</span> <span style="font-weight: bold;">chattr</span> <span style="color: #00BBBB;">──────────────────────────────────────────────────────────────────────</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── <span style="font-weight: bold;">Defaults for: </span><span style="color: #0000BB; font-weight: bold;">Default</span> ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Prompt:</span></span>
<span><span class="co">#&gt; • <span style="color: #008700;">Use the R language, the tidyverse, and tidymodels</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Model</span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Provider:</span> <span style="color: #0000BB;">LlamaGPT</span></span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Path/URL:</span> <span style="color: #0000BB;">~/LlamaGPTJ-chat/build/bin/chat</span></span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Model:</span> <span style="color: #0000BB;">~/ggml-gpt4all-j-v1.3-groovy.bin</span></span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Label:</span> <span style="color: #0000BB;">GPT4ALL 1.3 (LlamaGPT)</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Model Arguments:</span></span>
<span><span class="co">#&gt; • threads: <span style="color: #0000BB;">4</span></span></span>
<span><span class="co">#&gt; • temp: <span style="color: #0000BB;">0.01</span></span></span>
<span><span class="co">#&gt; • n_predict: <span style="color: #0000BB;">1000</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Context:</span></span>
<span><span class="co">#&gt; Max Data Files: <span style="color: #0000BB;">0</span></span></span>
<span><span class="co">#&gt; Max Data Frames: <span style="color: #0000BB;">0</span></span></span>
<span><span class="co">#&gt; <span style="color: #BB0000;">✖</span> Chat History</span></span>
<span><span class="co">#&gt; <span style="color: #BB0000;">✖</span> Document contents</span></span></code></pre></div>
<p>If either, or both, paths are incorrect, correct them by updating the
<code>path</code>, and <code>model</code> arguments in
<code><a href="../reference/chattr_defaults.html">chattr_defaults()</a></code></p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/chattr_defaults.html">chattr_defaults</a></span><span class="op">(</span>path <span class="op">=</span> <span class="st">"[path to compiled program]"</span>, model <span class="op">=</span> <span class="st">"[path to model]"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; <span style="color: #00BBBB;">──</span> <span style="font-weight: bold;">chattr</span> <span style="color: #00BBBB;">──────────────────────────────────────────────────────────────────────</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── <span style="font-weight: bold;">Defaults for: </span><span style="color: #0000BB; font-weight: bold;">Default</span> ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Prompt:</span></span>
<span><span class="co">#&gt; • <span style="color: #008700;">Use the R language, the tidyverse, and tidymodels</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Model</span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Provider:</span> <span style="color: #0000BB;">LlamaGPT</span></span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Path/URL:</span> <span style="color: #0000BB;">[path to compiled program]</span></span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Model:</span> <span style="color: #0000BB;">[path to model]</span></span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Label:</span> <span style="color: #0000BB;">GPT4ALL 1.3 (LlamaGPT)</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Model Arguments:</span></span>
<span><span class="co">#&gt; • threads: <span style="color: #0000BB;">4</span></span></span>
<span><span class="co">#&gt; • temp: <span style="color: #0000BB;">0.01</span></span></span>
<span><span class="co">#&gt; • n_predict: <span style="color: #0000BB;">1000</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Context:</span></span>
<span><span class="co">#&gt; Max Data Files: <span style="color: #0000BB;">0</span></span></span>
<span><span class="co">#&gt; Max Data Frames: <span style="color: #0000BB;">0</span></span></span>
<span><span class="co">#&gt; <span style="color: #BB0000;">✖</span> Chat History</span></span>
<span><span class="co">#&gt; <span style="color: #BB0000;">✖</span> Document contents</span></span></code></pre></div>
<p>So that you do not need to change those defaults every time you start
a new R session, use <code><a href="../reference/chattr_defaults_save.html">chattr_defaults_save()</a></code>. That will
create a YAML file in your working directory. <code>chattr</code> will
use that file to override the defaults.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/chattr_defaults_save.html">chattr_defaults_save</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>Use <code><a href="../reference/chattr_test.html">chattr_test()</a></code> to confirm that the
<code>chattr</code> is able to communicate with LLamaGPT-Chat, and that
the model it will use with your R session is accessible</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/chattr_test.html">chattr_test</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; ✔ Model started sucessfully</span></span>
<span><span class="co">#&gt; ✔ Model session closed sucessfully</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="model-arguments">Model Arguments<a class="anchor" aria-label="anchor" href="#model-arguments"></a>
</h2>
<p>The arguments sent to the model can be modified in
<code><a href="../reference/chattr_defaults.html">chattr_defaults()</a></code> by modifying the
<code>model_arguments</code> argument. It expects a list object. Here
are the arguments it sets by default:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/chattr_defaults.html">chattr_defaults</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">model_arguments</span></span>
<span><span class="co">#&gt; $threads</span></span>
<span><span class="co">#&gt; [1] 4</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $temp</span></span>
<span><span class="co">#&gt; [1] 0.01</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $n_predict</span></span>
<span><span class="co">#&gt; [1] 1000</span></span></code></pre></div>
<p>Here is an example of adding <code>batch_size</code> to the
defaults:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/chattr_defaults.html">chattr_defaults</a></span><span class="op">(</span>model_arguments <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>batch_size <span class="op">=</span> <span class="fl">40</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; <span style="color: #00BBBB;">──</span> <span style="font-weight: bold;">chattr</span> <span style="color: #00BBBB;">──────────────────────────────────────────────────────────────────────</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── <span style="font-weight: bold;">Defaults for: </span><span style="color: #0000BB; font-weight: bold;">Default</span> ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Prompt:</span></span>
<span><span class="co">#&gt; • <span style="color: #008700;">Use the R language, the tidyverse, and tidymodels</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Model</span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Provider:</span> <span style="color: #0000BB;">LlamaGPT</span></span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Path/URL:</span> <span style="color: #0000BB;">[path to compiled program]</span></span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Model:</span> <span style="color: #0000BB;">[path to model]</span></span></span>
<span><span class="co">#&gt; • <span style="color: #000000;">Label:</span> <span style="color: #0000BB;">GPT4ALL 1.3 (LlamaGPT)</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Model Arguments:</span></span>
<span><span class="co">#&gt; • batch_size: <span style="color: #0000BB;">40</span></span></span>
<span><span class="co">#&gt; • threads: <span style="color: #0000BB;">4</span></span></span>
<span><span class="co">#&gt; • temp: <span style="color: #0000BB;">0.01</span></span></span>
<span><span class="co">#&gt; • n_predict: <span style="color: #0000BB;">1000</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Context:</span></span>
<span><span class="co">#&gt; Max Data Files: <span style="color: #0000BB;">0</span></span></span>
<span><span class="co">#&gt; Max Data Frames: <span style="color: #0000BB;">0</span></span></span>
<span><span class="co">#&gt; <span style="color: #BB0000;">✖</span> Chat History</span></span>
<span><span class="co">#&gt; <span style="color: #BB0000;">✖</span> Document contents</span></span></code></pre></div>
<p>To see the most current list of available model arguments go to: <a href="https://github.com/kuvaus/LlamaGPTJ-chat#detailed-command-list" class="external-link">Detailed
command list</a>.</p>
<p><strong>IMPORTANT</strong> - <code>chattr</code> passes the arguments
directly to LLamaGPT-chat as a command line flag, except
<code>model</code>. <code>chattr</code> will use the value in
<code>chattr_defaults(model = "[path to model]")</code> instead.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Edgar Ruiz, Posit Software, PBC.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
